{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your first LM model with Amazon SageMaker\n",
    "\n",
    "### Sentiment Analysis with `DistilBERT` and `imdb` dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)  \n",
    "2. [Environment and Permissions](#Environment-and-Permissions)\n",
    "3. [Preprocess - Tokenization of the dataset](#Preprocessing)   \n",
    "4. [Creating an Estimator and start a training job](#Creating-an-Estimator-and-start-a-training-job)  \n",
    "5. [Deploying the endpoint](#Deploying-the-endpoint)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to our end-to-end binary Text-Classification example. In this demo, we will use the Hugging Face `transformers` and `datasets` library and the SageMaker SDK to launch a SageMaker Training job and fine-tune a pre-trained transformer for binary text classification. In particular, we will use the pre-trained DistilBERT model and fine-tune it using the `imdb` dataset. To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "_**NOTE: You can run this demo in Sagemaker Studio, your local machine or Sagemaker Notebook Instances**. This notebook has been tested in SageMaker Studio with a PyTorch 1.13 Python 3.9 CPU Optimized Kernel_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment and Permissions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-2.14.3-py3-none-any.whl (519 kB)\n",
      "Collecting xxhash\n",
      "  Using cached xxhash-3.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets) (11.0.0)\n",
      "Collecting aiohttp\n",
      "  Using cached aiohttp-3.8.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Collecting huggingface-hub<1.0.0,>=0.14.0\n",
      "  Using cached huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.9/site-packages (from datasets) (2023.1.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from datasets) (2.28.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (228 kB)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Using cached yarl-1.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.4.0)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, multidict, frozenlist, filelock, async-timeout, yarl, huggingface-hub, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.14.3 filelock-3.12.2 frozenlist-1.4.0 huggingface-hub-0.16.4 multidict-6.0.4 xxhash-3.3.0 yarl-1.9.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.9/site-packages (2.132.0)\n",
      "Collecting sagemaker\n",
      "  Using cached sagemaker-2.175.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from sagemaker) (1.5.3)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.9/site-packages (from sagemaker) (3.20.2)\n",
      "Collecting PyYAML~=6.0\n",
      "  Using cached PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from sagemaker) (23.0)\n",
      "Collecting boto3<2.0,>=1.26.131\n",
      "  Using cached boto3-1.28.20-py3-none-any.whl (135 kB)\n",
      "Collecting jsonschema\n",
      "  Using cached jsonschema-4.19.0-py3-none-any.whl (83 kB)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.9/site-packages (from sagemaker) (2.2.1)\n",
      "Collecting tblib==1.7.0\n",
      "  Using cached tblib-1.7.0-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.9/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.9/site-packages (from sagemaker) (0.7.5)\n",
      "Collecting attrs<24,>=23.1.0\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.9/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.9/site-packages (from sagemaker) (4.13.0)\n",
      "Collecting platformdirs\n",
      "  Using cached platformdirs-3.10.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.9/site-packages (from sagemaker) (0.3.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.9/site-packages (from sagemaker) (1.23.5)\n",
      "Collecting botocore<1.32.0,>=1.31.20\n",
      "  Using cached botocore-1.31.20-py3-none-any.whl (11.1 MB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.9/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.9/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (0.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.13.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6\n",
      "  Using cached jsonschema_specifications-2023.7.1-py3-none-any.whl (17 kB)\n",
      "Collecting referencing>=0.28.4\n",
      "  Using cached referencing-0.30.2-py3-none-any.whl (25 kB)\n",
      "Collecting rpds-py>=0.7.1\n",
      "  Using cached rpds_py-0.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->sagemaker) (2022.7.1)\n",
      "Requirement already satisfied: dill>=0.3.6 in /opt/conda/lib/python3.9/site-packages (from pathos->sagemaker) (0.3.6)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.9/site-packages (from pathos->sagemaker) (1.7.6.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.9/site-packages (from pathos->sagemaker) (0.3.2)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /opt/conda/lib/python3.9/site-packages (from pathos->sagemaker) (0.70.14)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.9/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.9/site-packages (from botocore<1.32.0,>=1.31.20->boto3<2.0,>=1.26.131->sagemaker) (1.26.14)\n",
      "Installing collected packages: tblib, rpds-py, PyYAML, platformdirs, attrs, referencing, botocore, jsonschema-specifications, jsonschema, boto3, sagemaker\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 5.4.1\n",
      "    Uninstalling PyYAML-5.4.1:\n",
      "      Successfully uninstalled PyYAML-5.4.1\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 22.2.0\n",
      "    Uninstalling attrs-22.2.0:\n",
      "      Successfully uninstalled attrs-22.2.0\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.29.70\n",
      "    Uninstalling botocore-1.29.70:\n",
      "      Successfully uninstalled botocore-1.29.70\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.26.70\n",
      "    Uninstalling boto3-1.26.70:\n",
      "      Successfully uninstalled boto3-1.26.70\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.132.0\n",
      "    Uninstalling sagemaker-2.132.0:\n",
      "      Successfully uninstalled sagemaker-2.132.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.27.70 requires botocore==1.29.70, but you have botocore 1.31.20 which is incompatible.\n",
      "awscli 1.27.70 requires PyYAML<5.5,>=3.10, but you have pyyaml 6.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed PyYAML-6.0.1 attrs-23.1.0 boto3-1.28.20 botocore-1.31.20 jsonschema-4.19.0 jsonschema-specifications-2023.7.1 platformdirs-3.10.0 referencing-0.30.2 rpds-py-0.9.2 sagemaker-2.175.0 tblib-1.7.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.9/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers) (1.23.5)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2023.6.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Using cached safetensors-0.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.4.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (2022.12.7)\n",
      "Installing collected packages: tokenizers, safetensors, regex, transformers\n",
      "Successfully installed regex-2023.6.3 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.9/site-packages (0.16.0)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from accelerate) (23.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.9/site-packages (from accelerate) (1.13.1+cpu)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from accelerate) (1.23.5)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (4.4.0)\n",
      "Installing collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.16.0\n",
      "    Uninstalling accelerate-0.16.0:\n",
      "      Successfully uninstalled accelerate-0.16.0\n",
      "Successfully installed accelerate-0.21.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install -U sagemaker\n",
    "!pip install -U transformers\n",
    "!pip install -U accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::897205850961:role/service-role/AmazonSageMaker-ExecutionRole-20230414T104809\n",
      "sagemaker bucket: sagemaker-eu-central-1-897205850961\n",
      "sagemaker session region: eu-central-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import sagemaker.huggingface\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualizing our data\n",
    "We are using the `datasets` library to download the `imdb` [dataset](https://huggingface.co/datasets/imdb). The dataset consists of 25,000 highly polar movie reviews for training, and 25,000 for testing.\n",
    "Let's see how our dataset looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset, test_dataset = load_dataset(\"imdb\", split=[\"train\", \"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 25000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 25000\n",
       " }))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'It was great to see some of my favorite stars of 30 years ago including John Ritter, Ben Gazarra and Audrey Hepburn. They looked quite wonderful. But that was it. They were not given any characters or good lines to work with. I neither understood or cared what the characters were doing.<br /><br />Some of the smaller female roles were fine, Patty Henson and Colleen Camp were quite competent and confident in their small sidekick parts. They showed some talent and it is sad they didn\\'t go on to star in more and better films. Sadly, I didn\\'t think Dorothy Stratten got a chance to act in this her only important film role.<br /><br />The film appears to have some fans, and I was very open-minded when I started watching it. I am a big Peter Bogdanovich fan and I enjoyed his last movie, \"Cat\\'s Meow\" and all his early ones from \"Targets\" to \"Nickleodeon\". So, it really surprised me that I was barely able to keep awake watching this one.<br /><br />It is ironic that this movie is about a detective agency where the detectives and clients get romantically involved with each other. Five years later, Bogdanovich\\'s ex-girlfriend, Cybil Shepherd had a hit television series called \"Moonlighting\" stealing the story idea from Bogdanovich. Of course, there was a great difference in that the series relied on tons of witty dialogue, while this tries to make do with slapstick and a few screwball lines.<br /><br />Bottom line: It ain\\'t no \"Paper Moon\" and only a very pale version of \"What\\'s Up, Doc\".',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset includes the 'text' field, which is the free text review, comment about the movie, and the 'label' field, which is a binary variable coded with value 0 for negative review and value 1 for positive ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Before you can train a model on a dataset, it needs to be preprocessed into the expected model input format. Whether your data is text, images, or audio, they need to be converted and assembled into batches of tensors.\n",
    "Text, use a [Tokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer) to convert text into a sequence of tokens, create a numerical representation of the tokens, and assemble them into tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25000/25000 [00:27<00:00, 911.75 examples/s] \n",
      "Map: 100%|██████████| 10000/10000 [00:11<00:00, 869.47 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer_name = 'distilbert-base-uncased'\n",
    "dataset_name = 'imdb'\n",
    "# s3 key prefix for the data\n",
    "s3_prefix = 'samples/datasets/imdb'\n",
    "\n",
    "# download tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "# tokenizer helper function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# load dataset\n",
    "train_dataset, test_dataset = load_dataset(dataset_name, split=['train', 'test'])\n",
    "test_dataset = test_dataset.shuffle().select(range(10000)) # smaller the size for test dataset to 10k \n",
    "\n",
    "# tokenize dataset\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# set format for pytorch\n",
    "train_dataset =  train_dataset.rename_column(\"label\", \"labels\")\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "train_dataset = train_dataset.remove_columns(\"text\")\n",
    "test_dataset = test_dataset.remove_columns(\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Uploading data to `sagemaker_session_bucket`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "After we processed the `datasets` we are going to use the new `FileSystem` [integration](https://huggingface.co/docs/datasets/filesystems.html) to upload our dataset to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 25000/25000 [00:02<00:00, 11393.15 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:01<00:00, 9883.54 examples/s] \n"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "train_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
    "test_dataset.save_to_disk(test_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Estimator and starting a training job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "From all supported models from Hugging Face, we selected https://huggingface.co/distilbert-base-uncased. It is light (\"distilled\") version of the BERT model, introduced in [this paper](https://arxiv.org/abs/1910.01108), where authors claim to have reduced the size of the model by 40% compared to BERT-base, while retaining 97%\n",
    "of its language understanding capabilities and being 60% faster. Size reduction and faster execution helps to reduce the footprint of the instance required for training and inference, reducing trainig time.\n",
    "According to the model card in hugging face, the raw model can be used for next sentence prediction but it's mostly intended to be fine-tuned on a downstream task like as sequence classification, token classification or question answering.\n",
    "In our case we will fine-tune the model to binary classify the sentence as brining positive or negative sentiment.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model Training (Model Fine-Tuning)\n",
    "Language Models size ranges from hundreds of million to hundreds of billion of parameters. For example BERT-base consists of 110M parameters. As training such models can take weeks, months ore much more, it is common practise to start from a pre-trained model and tune the network parameters (called model fine-tuning) using domain specific datasets with supervised learning. \n",
    "\n",
    "This is what we will do in the following. As you will see, the training (fine-tuning) on the emotion data set, takes just about 12 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "training_input_path = \"s3://{}/{}/train\".format(sess.default_bucket(), s3_prefix)\n",
    "test_input_path = \"s3://{}/{}/test\".format(sess.default_bucket(), s3_prefix)\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters = {\n",
    "    \"epochs\": 1,\n",
    "    \"train_batch_size\": 32,\n",
    "    \"model_name\": \"distilbert-base-uncased\",\n",
    "    \"learning_rate\": 0.00003,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point=\"train_script.py\",\n",
    "    source_dir=\"./scripts\",\n",
    "    instance_type=\"ml.g4dn.12xlarge\",\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    transformers_version=\"4.26\",\n",
    "    pytorch_version=\"1.13\",\n",
    "    py_version=\"py39\",\n",
    "    hyperparameters=hyperparameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-pytorch-training-2023-08-07-14-51-17-633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-07 14:51:18 Starting - Starting the training job...\n",
      "2023-08-07 14:51:33 Starting - Preparing the instances for training.........\n",
      "2023-08-07 14:53:12 Downloading - Downloading input data\n",
      "2023-08-07 14:53:12 Training - Downloading the training image...........................\n",
      "2023-08-07 14:57:23 Training - Training image download completed. Training in progress....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-08-07 14:58:08,582 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-08-07 14:58:08,617 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-07 14:58:08,628 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-08-07 14:58:08,630 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-08-07 14:58:08,871 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-07 14:58:08,918 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-07 14:58:08,964 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-07 14:58:08,975 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 1,\n",
      "        \"learning_rate\": 3e-05,\n",
      "        \"model_name\": \"distilbert-base-uncased\",\n",
      "        \"train_batch_size\": 32\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2023-08-07-14-51-17-633\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-central-1-897205850961/huggingface-pytorch-training-2023-08-07-14-51-17-633/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_script\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_script.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"learning_rate\":3e-05,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_script.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.12xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_script\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-central-1-897205850961/huggingface-pytorch-training-2023-08-07-14-51-17-633/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"learning_rate\":3e-05,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-pytorch-training-2023-08-07-14-51-17-633\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-central-1-897205850961/huggingface-pytorch-training-2023-08-07-14-51-17-633/source/sourcedir.tar.gz\",\"module_name\":\"train_script\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_script.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--learning_rate\",\"3e-05\",\"--model_name\",\"distilbert-base-uncased\",\"--train_batch_size\",\"32\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=3e-05\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 train_script.py --epochs 1 --learning_rate 3e-05 --model_name distilbert-base-uncased --train_batch_size 32\u001b[0m\n",
      "\u001b[34m[2023-08-07 14:58:10.668: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-08-07 14:58:10,673 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2023-08-07 14:58:10,697 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m2023-08-07 14:58:13,329 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m2023-08-07 14:58:13,330 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json: 100%|██████████| 483/483 [00:00<00:00, 69.9kB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/268M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  16%|█▌        | 41.9M/268M [00:00<00:00, 411MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  35%|███▌      | 94.4M/268M [00:00<00:00, 457MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  55%|█████▍    | 147M/268M [00:00<00:00, 465MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  74%|███████▍  | 199M/268M [00:00<00:00, 481MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  94%|█████████▍| 252M/268M [00:00<00:00, 496MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";: 100%|██████████| 268M/268M [00:00<00:00, 472MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mDownloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 13.0kB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 42.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 1.70MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 1.70MB/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m***** Running training *****\u001b[0m\n",
      "\u001b[34m***** Running training *****\u001b[0m\n",
      "\u001b[34mNum examples = 25000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\u001b[0m\n",
      "\u001b[34mNum examples = 25000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\u001b[0m\n",
      "\u001b[34mGradient Accumulation steps = 1\n",
      "  Total optimization steps = 196\u001b[0m\n",
      "\u001b[34mGradient Accumulation steps = 1\n",
      "  Total optimization steps = 196\u001b[0m\n",
      "\u001b[34mNumber of trainable parameters = 66955010\u001b[0m\n",
      "\u001b[34mNumber of trainable parameters = 66955010\u001b[0m\n",
      "\u001b[34m0%|          | 0/196 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-08-07 14:58:18.145: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-08-07 14:58:18,151 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[2023-08-07 14:58:18.179 algo-1:89 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-08-07 14:58:18.211 algo-1:89 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-08-07 14:58:18.211 algo-1:89 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-08-07 14:58:18.211 algo-1:89 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-08-07 14:58:18.212 algo-1:89 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-08-07 14:58:18.212 algo-1:89 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34m1%|          | 1/196 [00:07<25:09,  7.74s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 2/196 [00:09<13:14,  4.10s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 3/196 [00:10<09:25,  2.93s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 4/196 [00:12<07:37,  2.38s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 5/196 [00:13<06:36,  2.08s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 6/196 [00:15<06:00,  1.90s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 7/196 [00:17<05:37,  1.78s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 8/196 [00:18<05:22,  1.71s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 9/196 [00:20<05:11,  1.67s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 10/196 [00:21<05:03,  1.63s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 11/196 [00:23<04:57,  1.61s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 12/196 [00:24<04:52,  1.59s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 13/196 [00:26<04:48,  1.58s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 14/196 [00:27<04:46,  1.57s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 15/196 [00:29<04:43,  1.57s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 16/196 [00:31<04:41,  1.56s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 17/196 [00:32<04:40,  1.56s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 18/196 [00:34<04:38,  1.56s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 19/196 [00:35<04:37,  1.57s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 20/196 [00:37<04:36,  1.57s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 21/196 [00:38<04:34,  1.57s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 22/196 [00:40<04:32,  1.57s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 23/196 [00:41<04:31,  1.57s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 24/196 [00:43<04:30,  1.57s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 25/196 [00:45<04:29,  1.58s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 26/196 [00:46<04:28,  1.58s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 27/196 [00:48<04:26,  1.58s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 28/196 [00:49<04:24,  1.58s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 29/196 [00:51<04:23,  1.58s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 30/196 [00:53<04:22,  1.58s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 31/196 [00:54<04:20,  1.58s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 32/196 [00:56<04:20,  1.59s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 33/196 [00:57<04:18,  1.59s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 34/196 [00:59<04:17,  1.59s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 35/196 [01:01<04:15,  1.59s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 36/196 [01:02<04:14,  1.59s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 37/196 [01:04<04:13,  1.60s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 38/196 [01:05<04:11,  1.59s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 39/196 [01:07<04:09,  1.59s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 40/196 [01:08<04:08,  1.59s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 41/196 [01:10<04:06,  1.59s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 42/196 [01:12<04:05,  1.59s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 43/196 [01:13<04:03,  1.59s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 44/196 [01:15<04:02,  1.59s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 45/196 [01:16<04:00,  1.60s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 46/196 [01:18<04:00,  1.60s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 47/196 [01:20<03:58,  1.60s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 48/196 [01:21<03:57,  1.60s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 49/196 [01:23<03:56,  1.61s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 50/196 [01:24<03:54,  1.61s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 51/196 [01:26<03:53,  1.61s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 52/196 [01:28<03:52,  1.61s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 53/196 [01:29<03:51,  1.62s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 54/196 [01:31<03:50,  1.62s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 55/196 [01:33<03:48,  1.62s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 56/196 [01:34<03:48,  1.63s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 57/196 [01:36<03:46,  1.63s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 58/196 [01:38<03:45,  1.63s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 59/196 [01:39<03:43,  1.63s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 60/196 [01:41<03:42,  1.64s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 61/196 [01:42<03:41,  1.64s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 62/196 [01:44<03:39,  1.64s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 63/196 [01:46<03:38,  1.64s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 64/196 [01:47<03:36,  1.64s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 65/196 [01:49<03:35,  1.65s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 66/196 [01:51<03:34,  1.65s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 67/196 [01:52<03:33,  1.65s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 68/196 [01:54<03:31,  1.66s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 69/196 [01:56<03:30,  1.66s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 70/196 [01:57<03:28,  1.66s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 71/196 [01:59<03:27,  1.66s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 72/196 [02:01<03:26,  1.66s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 73/196 [02:02<03:24,  1.66s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 74/196 [02:04<03:22,  1.66s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 75/196 [02:06<03:21,  1.67s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 76/196 [02:07<03:19,  1.67s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 77/196 [02:09<03:17,  1.66s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 78/196 [02:11<03:15,  1.66s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 79/196 [02:12<03:13,  1.66s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 80/196 [02:14<03:12,  1.66s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 81/196 [02:16<03:11,  1.66s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 82/196 [02:17<03:10,  1.67s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 83/196 [02:19<03:08,  1.67s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 84/196 [02:21<03:06,  1.67s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 85/196 [02:22<03:06,  1.68s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 86/196 [02:24<03:04,  1.67s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 87/196 [02:26<03:03,  1.68s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 88/196 [02:27<03:01,  1.68s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 89/196 [02:29<03:00,  1.68s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 90/196 [02:31<02:58,  1.69s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 91/196 [02:32<02:57,  1.69s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 92/196 [02:34<02:55,  1.69s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 93/196 [02:36<02:54,  1.69s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 94/196 [02:38<02:52,  1.69s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 95/196 [02:39<02:50,  1.69s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 96/196 [02:41<02:49,  1.69s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 97/196 [02:43<02:47,  1.70s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 98/196 [02:44<02:46,  1.70s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 99/196 [02:46<02:45,  1.70s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 100/196 [02:48<02:43,  1.70s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 101/196 [02:49<02:41,  1.70s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 102/196 [02:51<02:39,  1.70s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 103/196 [02:53<02:38,  1.70s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 104/196 [02:55<02:36,  1.70s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 105/196 [02:56<02:35,  1.71s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 106/196 [02:58<02:33,  1.71s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 107/196 [03:00<02:32,  1.71s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 108/196 [03:01<02:30,  1.71s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 109/196 [03:03<02:28,  1.71s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 110/196 [03:05<02:27,  1.71s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 111/196 [03:07<02:25,  1.71s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 112/196 [03:08<02:23,  1.71s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 113/196 [03:10<02:22,  1.72s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 114/196 [03:12<02:20,  1.71s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 115/196 [03:13<02:18,  1.71s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 116/196 [03:15<02:16,  1.71s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 117/196 [03:17<02:15,  1.71s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 118/196 [03:19<02:13,  1.71s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 119/196 [03:20<02:12,  1.72s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 120/196 [03:22<02:10,  1.72s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 121/196 [03:24<02:09,  1.72s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 122/196 [03:25<02:07,  1.72s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 123/196 [03:27<02:05,  1.72s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 124/196 [03:29<02:03,  1.72s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 125/196 [03:31<02:02,  1.72s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 126/196 [03:32<02:00,  1.72s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 127/196 [03:34<01:58,  1.72s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 128/196 [03:36<01:56,  1.72s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 129/196 [03:37<01:55,  1.72s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 130/196 [03:39<01:53,  1.72s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 131/196 [03:41<01:51,  1.72s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 132/196 [03:43<01:50,  1.72s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 133/196 [03:44<01:48,  1.72s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 134/196 [03:46<01:46,  1.72s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 135/196 [03:48<01:45,  1.72s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 136/196 [03:50<01:43,  1.72s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 137/196 [03:51<01:41,  1.72s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 138/196 [03:53<01:39,  1.72s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 139/196 [03:55<01:38,  1.73s/it]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 140/196 [03:56<01:36,  1.73s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 141/196 [03:58<01:35,  1.73s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 142/196 [04:00<01:33,  1.73s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 143/196 [04:02<01:31,  1.73s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 144/196 [04:03<01:30,  1.73s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 145/196 [04:05<01:28,  1.73s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 146/196 [04:07<01:26,  1.73s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 147/196 [04:09<01:25,  1.73s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 148/196 [04:10<01:23,  1.74s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 149/196 [04:12<01:21,  1.74s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 150/196 [04:14<01:19,  1.73s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 151/196 [04:16<01:17,  1.73s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 152/196 [04:17<01:16,  1.73s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 153/196 [04:19<01:14,  1.73s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 154/196 [04:21<01:12,  1.73s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 155/196 [04:22<01:10,  1.73s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 156/196 [04:24<01:09,  1.73s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 157/196 [04:26<01:07,  1.72s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 158/196 [04:28<01:05,  1.73s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 159/196 [04:29<01:04,  1.73s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 160/196 [04:31<01:02,  1.73s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 161/196 [04:33<01:00,  1.73s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 162/196 [04:35<00:58,  1.73s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 163/196 [04:36<00:56,  1.73s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 164/196 [04:38<00:55,  1.72s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 165/196 [04:40<00:53,  1.72s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 166/196 [04:41<00:51,  1.72s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 167/196 [04:43<00:49,  1.72s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 168/196 [04:45<00:48,  1.72s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 169/196 [04:47<00:46,  1.72s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 170/196 [04:48<00:44,  1.72s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 171/196 [04:50<00:42,  1.72s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 172/196 [04:52<00:41,  1.72s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 173/196 [04:53<00:39,  1.72s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 174/196 [04:55<00:37,  1.72s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 175/196 [04:57<00:36,  1.72s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 176/196 [04:59<00:34,  1.72s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 177/196 [05:00<00:32,  1.72s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 178/196 [05:02<00:30,  1.72s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 179/196 [05:04<00:29,  1.72s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 180/196 [05:05<00:27,  1.72s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 181/196 [05:07<00:25,  1.71s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 182/196 [05:09<00:23,  1.71s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 183/196 [05:11<00:22,  1.71s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 184/196 [05:12<00:20,  1.71s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 185/196 [05:14<00:18,  1.71s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 186/196 [05:16<00:17,  1.71s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 187/196 [05:17<00:15,  1.71s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 188/196 [05:19<00:13,  1.71s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 189/196 [05:21<00:11,  1.71s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 190/196 [05:23<00:10,  1.71s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 191/196 [05:24<00:08,  1.71s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 192/196 [05:26<00:06,  1.71s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 193/196 [05:28<00:05,  1.71s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 194/196 [05:29<00:03,  1.71s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 195/196 [05:31<00:01,  1.71s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 196/196 [05:32<00:00,  1.40s/it]\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 10000\n",
      "  Batch size = 256\u001b[0m\n",
      "\u001b[34mNum examples = 10000\n",
      "  Batch size = 256\u001b[0m\n",
      "\u001b[34m0%|          | 0/40 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m5%|▌         | 2/40 [00:01<00:24,  1.53it/s]#033[A\u001b[0m\n",
      "\u001b[34m8%|▊         | 3/40 [00:02<00:34,  1.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m10%|█         | 4/40 [00:03<00:38,  1.07s/it]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▎        | 5/40 [00:05<00:40,  1.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m15%|█▌        | 6/40 [00:06<00:40,  1.20s/it]#033[A\u001b[0m\n",
      "\u001b[34m18%|█▊        | 7/40 [00:07<00:40,  1.23s/it]#033[A\u001b[0m\n",
      "\u001b[34m20%|██        | 8/40 [00:09<00:40,  1.25s/it]#033[A\u001b[0m\n",
      "\u001b[34m22%|██▎       | 9/40 [00:10<00:39,  1.27s/it]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▌       | 10/40 [00:11<00:38,  1.28s/it]#033[A\u001b[0m\n",
      "\u001b[34m28%|██▊       | 11/40 [00:13<00:37,  1.29s/it]#033[A\u001b[0m\n",
      "\u001b[34m30%|███       | 12/40 [00:14<00:36,  1.29s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▎      | 13/40 [00:15<00:34,  1.29s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|███▌      | 14/40 [00:16<00:33,  1.30s/it]#033[A\u001b[0m\n",
      "\u001b[34m38%|███▊      | 15/40 [00:18<00:32,  1.30s/it]#033[A\u001b[0m\n",
      "\u001b[34m40%|████      | 16/40 [00:19<00:31,  1.30s/it]#033[A\u001b[0m\n",
      "\u001b[34m42%|████▎     | 17/40 [00:20<00:29,  1.30s/it]#033[A\u001b[0m\n",
      "\u001b[34m45%|████▌     | 18/40 [00:22<00:28,  1.30s/it]#033[A\u001b[0m\n",
      "\u001b[34m48%|████▊     | 19/40 [00:23<00:27,  1.30s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 20/40 [00:24<00:25,  1.30s/it]#033[A\u001b[0m\n",
      "\u001b[34m52%|█████▎    | 21/40 [00:26<00:24,  1.30s/it]#033[A\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 22/40 [00:27<00:23,  1.30s/it]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▊    | 23/40 [00:28<00:22,  1.30s/it]#033[A\u001b[0m\n",
      "\u001b[34m60%|██████    | 24/40 [00:29<00:20,  1.30s/it]#033[A\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 25/40 [00:31<00:19,  1.30s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 26/40 [00:32<00:18,  1.30s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 27/40 [00:33<00:16,  1.30s/it]#033[A\u001b[0m\n",
      "\u001b[34m70%|███████   | 28/40 [00:35<00:15,  1.30s/it]#033[A\u001b[0m\n",
      "\u001b[34m72%|███████▎  | 29/40 [00:36<00:14,  1.30s/it]#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 30/40 [00:37<00:12,  1.30s/it]#033[A\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 31/40 [00:39<00:11,  1.30s/it]#033[A\u001b[0m\n",
      "\u001b[34m80%|████████  | 32/40 [00:40<00:10,  1.30s/it]#033[A\u001b[0m\n",
      "\u001b[34m82%|████████▎ | 33/40 [00:41<00:09,  1.30s/it]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 34/40 [00:42<00:07,  1.30s/it]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 35/40 [00:44<00:06,  1.30s/it]#033[A\u001b[0m\n",
      "\u001b[34m90%|█████████ | 36/40 [00:45<00:05,  1.30s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▎| 37/40 [00:46<00:03,  1.30s/it]#033[A\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 38/40 [00:48<00:02,  1.30s/it]#033[A\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 39/40 [00:49<00:01,  1.30s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 40/40 [00:49<00:00,  1.03it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2450455129146576, 'eval_accuracy': 0.9037, 'eval_f1': 0.9001555209953344, 'eval_precision': 0.9325456498388829, 'eval_recall': 0.869939879759519, 'eval_runtime': 50.8988, 'eval_samples_per_second': 196.468, 'eval_steps_per_second': 0.786, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 196/196 [06:23<00:00,  1.40s/it]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 40/40 [00:49<00:00,  1.03it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m{'train_runtime': 383.1802, 'train_samples_per_second': 65.243, 'train_steps_per_second': 0.512, 'train_loss': 0.4675844542834224, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 196/196 [06:23<00:00,  1.40s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 196/196 [06:23<00:00,  1.95s/it]\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 10000\n",
      "  Batch size = 256\u001b[0m\n",
      "\u001b[34mNum examples = 10000\n",
      "  Batch size = 256\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m0%|          | 0/40 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 2/40 [00:01<00:24,  1.55it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 3/40 [00:02<00:33,  1.09it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 4/40 [00:03<00:38,  1.06s/it]\u001b[0m\n",
      "\u001b[34m12%|█▎        | 5/40 [00:05<00:39,  1.14s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 6/40 [00:06<00:40,  1.19s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 7/40 [00:07<00:40,  1.23s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 8/40 [00:09<00:39,  1.25s/it]\u001b[0m\n",
      "\u001b[34m22%|██▎       | 9/40 [00:10<00:39,  1.26s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 10/40 [00:11<00:38,  1.27s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 11/40 [00:12<00:37,  1.28s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 12/40 [00:14<00:35,  1.28s/it]\u001b[0m\n",
      "\u001b[34m32%|███▎      | 13/40 [00:15<00:34,  1.29s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 14/40 [00:16<00:33,  1.29s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 15/40 [00:18<00:32,  1.29s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 16/40 [00:19<00:31,  1.29s/it]\u001b[0m\n",
      "\u001b[34m42%|████▎     | 17/40 [00:20<00:29,  1.29s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 18/40 [00:22<00:28,  1.29s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 19/40 [00:23<00:27,  1.29s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 20/40 [00:24<00:25,  1.29s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▎    | 21/40 [00:25<00:24,  1.29s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 22/40 [00:27<00:23,  1.29s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▊    | 23/40 [00:28<00:22,  1.29s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 24/40 [00:29<00:20,  1.30s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 25/40 [00:31<00:19,  1.30s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 26/40 [00:32<00:18,  1.30s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 27/40 [00:33<00:16,  1.30s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 28/40 [00:34<00:15,  1.29s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▎  | 29/40 [00:36<00:14,  1.29s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 30/40 [00:37<00:12,  1.30s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 31/40 [00:38<00:11,  1.30s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 32/40 [00:40<00:10,  1.30s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▎ | 33/40 [00:41<00:09,  1.30s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 34/40 [00:42<00:07,  1.30s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 35/40 [00:44<00:06,  1.30s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 36/40 [00:45<00:05,  1.29s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▎| 37/40 [00:46<00:03,  1.29s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 38/40 [00:47<00:02,  1.32s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 39/40 [00:49<00:01,  1.31s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 40/40 [00:49<00:00,  1.02it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 40/40 [00:49<00:00,  1.24s/it]\u001b[0m\n",
      "\u001b[34m***** Eval results *****\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m2023-08-07 15:05:33,308 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-08-07 15:05:33,309 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-08-07 15:05:33,309 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-08-07 15:06:02 Uploading - Uploading generated training model\n",
      "2023-08-07 15:06:02 Completed - Training job completed\n",
      "Training seconds: 790\n",
      "Billable seconds: 790\n"
     ]
    }
   ],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit({\"train\": training_input_path, \"test\": test_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While waiting for training to complete, which will take about 12 minutes, you can move to the SageMaker Console, click on Training -> Training Jobs and you will see your training job (InProgress Status) that you started calling the fit method in the cell above.\n",
    "When the training is completed you can see your training Job in Completed Status. Clicking on the job name you are directed to a page reporting several details of your training job, in particular at bottom of the page you can find the location of the model artifact you just created. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the endpoint\n",
    "\n",
    "To deploy our endpoint, we call `deploy()` on our HuggingFace estimator object, passing in our desired number of instances and instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: huggingface-pytorch-training-2023-08-07-15-09-49-895\n",
      "INFO:sagemaker:Creating endpoint-config with name huggingface-pytorch-training-2023-08-07-15-09-49-895\n",
      "INFO:sagemaker:Creating endpoint with name huggingface-pytorch-training-2023-08-07-15-09-49-895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------!"
     ]
    }
   ],
   "source": [
    "predictor = huggingface_estimator.deploy(initial_instance_count=1, instance_type=\"ml.g4dn.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use the returned predictor object to call the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.9557088017463684}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_input = {\n",
    "    \"inputs\": \"This is the best movie ever made in history, an absolute sculpted work of art that depicts every emotion of human existence, from suffering, to courage to love, in front of the background of political astuteness and socio-hierarchal analysis.\"\n",
    "}\n",
    "\n",
    "predictor.predict(sentiment_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.9047076106071472}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_input = {\n",
    "    \"inputs\": \"Another bloated film that gets all the history wrong, turns all of the characters into stick figures and makes piles of money for the star.\"\n",
    "}\n",
    "\n",
    "predictor.predict(sentiment_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we delete the endpoint again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.13 Python 3.9 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-central-1:936697816551:image/pytorch-1.13-cpu-py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
