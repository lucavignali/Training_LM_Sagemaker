{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your first LM model with Amazon SageMaker\n",
    "\n",
    "### Sentiment Analysis with `DistilBERT` and `imdb` dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)  \n",
    "2. [Environment and Permissions](#Environment-and-Permissions)\n",
    "3. [Preprocess - Tokenization of the dataset](#Preprocessing)   \n",
    "4. [Creating an Estimator and start a training job](#Creating-an-Estimator-and-start-a-training-job)  \n",
    "5. [Deploying the endpoint](#Deploying-the-endpoint)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to our end-to-end binary Text-Classification example. In this demo, we will use the Hugging Face `transformers` and `datasets` library and the SageMaker SDK to launch a SageMaker Training job and fine-tune a pre-trained transformer for binary text classification. In particular, we will use the pre-trained DistilBERT model and fine-tune it using the `imdb` dataset. To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "_**NOTE: You can run this demo in Sagemaker Studio, your local machine or Sagemaker Notebook Instances**. This notebook has been tested in SageMaker Studio with a PyTorch 1.13 Python 3.9 CPU Optimized Kernel_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment and Permissions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.9/site-packages (2.14.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/conda/lib/python3.9/site-packages (from datasets) (0.16.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from datasets) (2.28.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.9/site-packages (from datasets) (2023.1.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.9/site-packages (2.132.0)\n",
      "Collecting sagemaker\n",
      "  Using cached sagemaker-2.174.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.9/site-packages (from sagemaker) (3.20.2)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.9/site-packages (from sagemaker) (0.3.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from sagemaker) (1.5.3)\n",
      "Collecting PyYAML~=6.0\n",
      "  Using cached PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB)\n",
      "Collecting tblib==1.7.0\n",
      "  Using cached tblib-1.7.0-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.9/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from sagemaker) (23.0)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.9/site-packages (from sagemaker) (2.2.1)\n",
      "Collecting boto3<2.0,>=1.26.131\n",
      "  Using cached boto3-1.28.19-py3-none-any.whl (135 kB)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.9/site-packages (from sagemaker) (1.0.1)\n",
      "Collecting platformdirs\n",
      "  Using cached platformdirs-3.10.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.9/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.9/site-packages (from sagemaker) (1.23.5)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.9/site-packages (from sagemaker) (4.13.0)\n",
      "Collecting attrs<24,>=23.1.0\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting jsonschema\n",
      "  Using cached jsonschema-4.18.6-py3-none-any.whl (83 kB)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.9/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.9/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (1.0.1)\n",
      "Collecting botocore<1.32.0,>=1.31.19\n",
      "  Using cached botocore-1.31.19-py3-none-any.whl (11.1 MB)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.13.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Collecting rpds-py>=0.7.1\n",
      "  Using cached rpds_py-0.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Collecting referencing>=0.28.4\n",
      "  Using cached referencing-0.30.1-py3-none-any.whl (25 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6\n",
      "  Using cached jsonschema_specifications-2023.7.1-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->sagemaker) (2022.7.1)\n",
      "Requirement already satisfied: dill>=0.3.6 in /opt/conda/lib/python3.9/site-packages (from pathos->sagemaker) (0.3.6)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /opt/conda/lib/python3.9/site-packages (from pathos->sagemaker) (0.70.14)\n",
      "Requirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.9/site-packages (from pathos->sagemaker) (0.3.2)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.9/site-packages (from pathos->sagemaker) (1.7.6.6)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.9/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.9/site-packages (from botocore<1.32.0,>=1.31.19->boto3<2.0,>=1.26.131->sagemaker) (1.26.14)\n",
      "Installing collected packages: tblib, rpds-py, PyYAML, platformdirs, attrs, referencing, botocore, jsonschema-specifications, jsonschema, boto3, sagemaker\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 5.4.1\n",
      "    Uninstalling PyYAML-5.4.1:\n",
      "      Successfully uninstalled PyYAML-5.4.1\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 22.2.0\n",
      "    Uninstalling attrs-22.2.0:\n",
      "      Successfully uninstalled attrs-22.2.0\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.29.70\n",
      "    Uninstalling botocore-1.29.70:\n",
      "      Successfully uninstalled botocore-1.29.70\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.26.70\n",
      "    Uninstalling boto3-1.26.70:\n",
      "      Successfully uninstalled boto3-1.26.70\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.132.0\n",
      "    Uninstalling sagemaker-2.132.0:\n",
      "      Successfully uninstalled sagemaker-2.132.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.27.70 requires botocore==1.29.70, but you have botocore 1.31.19 which is incompatible.\n",
      "awscli 1.27.70 requires PyYAML<5.5,>=3.10, but you have pyyaml 6.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed PyYAML-6.0.1 attrs-23.1.0 boto3-1.28.19 botocore-1.31.19 jsonschema-4.18.6 jsonschema-specifications-2023.7.1 platformdirs-3.10.0 referencing-0.30.1 rpds-py-0.9.2 sagemaker-2.174.0 tblib-1.7.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::477886989750:role/service-role/AmazonSageMaker-ExecutionRole-20210525T143245\n",
      "sagemaker bucket: sagemaker-us-east-1-477886989750\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualizing our data\n",
    "We are using the `datasets` library to download the `imdb` [dataset](https://huggingface.co/datasets/imdb). The dataset consists of 25,000 highly polar movie reviews for training, and 25,000 for testing.\n",
    "Let's see how our dataset looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading builder script: 100%|██████████| 4.31k/4.31k [00:00<00:00, 1.86MB/s]\n",
      "Downloading metadata: 100%|██████████| 2.17k/2.17k [00:00<00:00, 1.04MB/s]\n",
      "Downloading readme: 100%|██████████| 7.59k/7.59k [00:00<00:00, 4.39MB/s]\n",
      "Downloading data: 100%|██████████| 84.1M/84.1M [00:04<00:00, 19.0MB/s]\n",
      "Generating train split: 100%|██████████| 25000/25000 [00:09<00:00, 2617.00 examples/s]\n",
      "Generating test split: 100%|██████████| 25000/25000 [00:12<00:00, 1989.25 examples/s]\n",
      "Generating unsupervised split: 100%|██████████| 50000/50000 [00:10<00:00, 4734.58 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset, test_dataset = load_dataset(\"imdb\", split=[\"train\", \"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 25000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 25000\n",
       " }))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'It was great to see some of my favorite stars of 30 years ago including John Ritter, Ben Gazarra and Audrey Hepburn. They looked quite wonderful. But that was it. They were not given any characters or good lines to work with. I neither understood or cared what the characters were doing.<br /><br />Some of the smaller female roles were fine, Patty Henson and Colleen Camp were quite competent and confident in their small sidekick parts. They showed some talent and it is sad they didn\\'t go on to star in more and better films. Sadly, I didn\\'t think Dorothy Stratten got a chance to act in this her only important film role.<br /><br />The film appears to have some fans, and I was very open-minded when I started watching it. I am a big Peter Bogdanovich fan and I enjoyed his last movie, \"Cat\\'s Meow\" and all his early ones from \"Targets\" to \"Nickleodeon\". So, it really surprised me that I was barely able to keep awake watching this one.<br /><br />It is ironic that this movie is about a detective agency where the detectives and clients get romantically involved with each other. Five years later, Bogdanovich\\'s ex-girlfriend, Cybil Shepherd had a hit television series called \"Moonlighting\" stealing the story idea from Bogdanovich. Of course, there was a great difference in that the series relied on tons of witty dialogue, while this tries to make do with slapstick and a few screwball lines.<br /><br />Bottom line: It ain\\'t no \"Paper Moon\" and only a very pale version of \"What\\'s Up, Doc\".',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Before you can train a model on a dataset, it needs to be preprocessed into the expected model input format. Whether your data is text, images, or audio, they need to be converted and assembled into batches of tensors.\n",
    "Text, use a [Tokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer) to convert text into a sequence of tokens, create a numerical representation of the tokens, and assemble them into tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25000/25000 [00:30<00:00, 824.89 examples/s]\n",
      "Map: 100%|██████████| 10000/10000 [00:11<00:00, 870.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer_name = 'distilbert-base-uncased'\n",
    "dataset_name = 'imdb'\n",
    "\n",
    "# download tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "# tokenizer helper function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# load dataset\n",
    "train_dataset, test_dataset = load_dataset(dataset_name, split=['train', 'test'])\n",
    "test_dataset = test_dataset.shuffle().select(range(10000)) # smaller the size for test dataset to 10k \n",
    "\n",
    "# tokenize dataset\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# set format for pytorch\n",
    "train_dataset =  train_dataset.rename_column(\"label\", \"labels\")\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "train_dataset = train_dataset.remove_columns(\"text\")\n",
    "test_dataset = test_dataset.remove_columns(\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Uploading data to `sagemaker_session_bucket`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "After we processed the `datasets` we are going to use the new `FileSystem` [integration](https://huggingface.co/docs/datasets/filesystems.html) to upload our dataset to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 25000/25000 [00:02<00:00, 10760.41 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 10109.20 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "train_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
    "test_dataset.save_to_disk(test_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Estimator and starting a training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "training_input_path = \"s3://{}/{}/train\".format(sess.default_bucket(), s3_prefix)\n",
    "test_input_path = \"s3://{}/{}/test\".format(sess.default_bucket(), s3_prefix)\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters = {\n",
    "    \"epochs\": 1,\n",
    "    \"train_batch_size\": 32,\n",
    "    \"model_name\": \"distilbert-base-uncased\",\n",
    "    \"learning_rate\": 0.00003,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"./scripts\",\n",
    "    instance_type=\"ml.g4dn.12xlarge\",\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    transformers_version=\"4.26\",\n",
    "    pytorch_version=\"1.13\",\n",
    "    py_version=\"py39\",\n",
    "    hyperparameters=hyperparameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: huggingface-pytorch-training-2023-08-04-15-27-25-744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-04 15:27:26 Starting - Starting the training job...\n",
      "2023-08-04 15:27:52 Starting - Preparing the instances for training.........\n",
      "2023-08-04 15:29:19 Downloading - Downloading input data\n",
      "2023-08-04 15:29:19 Training - Downloading the training image...............\n",
      "2023-08-04 15:31:30 Training - Training image download completed. Training in progress.....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-08-04 15:32:25,374 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-08-04 15:32:25,409 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-04 15:32:25,419 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-08-04 15:32:25,421 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-08-04 15:32:25,691 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-04 15:32:25,738 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-04 15:32:25,784 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-04 15:32:25,796 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 1,\n",
      "        \"learning_rate\": 3e-05,\n",
      "        \"model_name\": \"distilbert-base-uncased\",\n",
      "        \"train_batch_size\": 32\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2023-08-04-15-27-25-744\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-477886989750/huggingface-pytorch-training-2023-08-04-15-27-25-744/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"learning_rate\":3e-05,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.12xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-477886989750/huggingface-pytorch-training-2023-08-04-15-27-25-744/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"learning_rate\":3e-05,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-pytorch-training-2023-08-04-15-27-25-744\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-477886989750/huggingface-pytorch-training-2023-08-04-15-27-25-744/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--learning_rate\",\"3e-05\",\"--model_name\",\"distilbert-base-uncased\",\"--train_batch_size\",\"32\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=3e-05\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 train.py --epochs 1 --learning_rate 3e-05 --model_name distilbert-base-uncased --train_batch_size 32\u001b[0m\n",
      "\u001b[34m[2023-08-04 15:32:27.496: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-08-04 15:32:27,501 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2023-08-04 15:32:27,525 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m2023-08-04 15:32:30,203 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m2023-08-04 15:32:30,203 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json: 100%|██████████| 483/483 [00:00<00:00, 70.8kB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/268M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  12%|█▏        | 31.5M/268M [00:00<00:00, 311MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  27%|██▋       | 73.4M/268M [00:00<00:00, 365MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  47%|████▋     | 126M/268M [00:00<00:00, 396MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  67%|██████▋   | 178M/268M [00:00<00:00, 410MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  86%|████████▌ | 231M/268M [00:00<00:00, 416MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";: 100%|██████████| 268M/268M [00:00<00:00, 405MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mDownloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 13.1kB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 18.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 45.2MB/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m***** Running training *****\u001b[0m\n",
      "\u001b[34m***** Running training *****\u001b[0m\n",
      "\u001b[34mNum examples = 25000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\u001b[0m\n",
      "\u001b[34mNum examples = 25000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\u001b[0m\n",
      "\u001b[34mTotal train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 196\u001b[0m\n",
      "\u001b[34mTotal train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 196\u001b[0m\n",
      "\u001b[34mNumber of trainable parameters = 66955010\u001b[0m\n",
      "\u001b[34mNumber of trainable parameters = 66955010\u001b[0m\n",
      "\u001b[34m0%|          | 0/196 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-08-04 15:32:34.292: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-08-04 15:32:34,296 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[2023-08-04 15:32:34.324 algo-1:89 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-08-04 15:32:34.354 algo-1:89 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-08-04 15:32:34.354 algo-1:89 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-08-04 15:32:34.355 algo-1:89 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-08-04 15:32:34.355 algo-1:89 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-08-04 15:32:34.355 algo-1:89 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34m1%|          | 1/196 [00:07<24:46,  7.62s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 2/196 [00:09<13:01,  4.03s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 3/196 [00:10<09:16,  2.89s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 4/196 [00:12<07:31,  2.35s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 5/196 [00:13<06:31,  2.05s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 6/196 [00:15<05:55,  1.87s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 7/196 [00:16<05:32,  1.76s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 8/196 [00:18<05:16,  1.69s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 9/196 [00:19<05:05,  1.64s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 10/196 [00:21<04:58,  1.60s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 11/196 [00:22<04:51,  1.58s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 12/196 [00:24<04:47,  1.56s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 13/196 [00:25<04:44,  1.56s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 14/196 [00:27<04:41,  1.55s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 15/196 [00:28<04:39,  1.54s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 16/196 [00:30<04:37,  1.54s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 17/196 [00:32<04:35,  1.54s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 18/196 [00:33<04:33,  1.54s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 19/196 [00:35<04:32,  1.54s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 20/196 [00:36<04:31,  1.55s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 21/196 [00:38<04:30,  1.54s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 22/196 [00:39<04:28,  1.54s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 23/196 [00:41<04:27,  1.54s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 24/196 [00:42<04:25,  1.54s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 25/196 [00:44<04:23,  1.54s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 26/196 [00:45<04:22,  1.55s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 27/196 [00:47<04:20,  1.54s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 28/196 [00:49<04:19,  1.54s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 29/196 [00:50<04:17,  1.54s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 30/196 [00:52<04:16,  1.55s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 31/196 [00:53<04:15,  1.55s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 32/196 [00:55<04:14,  1.55s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 33/196 [00:56<04:13,  1.55s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 34/196 [00:58<04:11,  1.55s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 35/196 [00:59<04:10,  1.55s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 36/196 [01:01<04:09,  1.56s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 37/196 [01:03<04:08,  1.56s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 38/196 [01:04<04:06,  1.56s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 39/196 [01:06<04:05,  1.56s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 40/196 [01:07<04:05,  1.57s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 41/196 [01:09<04:03,  1.57s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 42/196 [01:10<04:01,  1.57s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 43/196 [01:12<03:59,  1.57s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 44/196 [01:14<03:58,  1.57s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 45/196 [01:15<03:56,  1.57s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 46/196 [01:17<03:56,  1.57s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 47/196 [01:18<03:54,  1.57s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 48/196 [01:20<03:53,  1.58s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 49/196 [01:21<03:52,  1.58s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 50/196 [01:23<03:50,  1.58s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 51/196 [01:25<03:49,  1.58s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 52/196 [01:26<03:48,  1.59s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 53/196 [01:28<03:47,  1.59s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 54/196 [01:29<03:46,  1.60s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 55/196 [01:31<03:44,  1.59s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 56/196 [01:33<03:43,  1.59s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 57/196 [01:34<03:41,  1.60s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 58/196 [01:36<03:39,  1.59s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 59/196 [01:37<03:37,  1.59s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 60/196 [01:39<03:36,  1.59s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 61/196 [01:41<03:34,  1.59s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 62/196 [01:42<03:33,  1.59s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 63/196 [01:44<03:32,  1.59s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 64/196 [01:45<03:31,  1.60s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 65/196 [01:47<03:30,  1.60s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 66/196 [01:49<03:28,  1.61s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 67/196 [01:50<03:27,  1.61s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 68/196 [01:52<03:25,  1.61s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 69/196 [01:53<03:24,  1.61s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 70/196 [01:55<03:22,  1.61s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 71/196 [01:57<03:21,  1.61s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 72/196 [01:58<03:20,  1.62s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 73/196 [02:00<03:19,  1.62s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 74/196 [02:02<03:18,  1.62s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 75/196 [02:03<03:16,  1.62s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 76/196 [02:05<03:14,  1.62s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 77/196 [02:06<03:13,  1.62s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 78/196 [02:08<03:12,  1.63s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 79/196 [02:10<03:11,  1.63s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 80/196 [02:11<03:09,  1.64s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 81/196 [02:13<03:08,  1.64s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 82/196 [02:15<03:06,  1.64s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 83/196 [02:16<03:05,  1.64s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 84/196 [02:18<03:03,  1.64s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 85/196 [02:20<03:02,  1.65s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 86/196 [02:21<03:01,  1.65s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 87/196 [02:23<03:00,  1.65s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 88/196 [02:25<02:57,  1.65s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 89/196 [02:26<02:56,  1.65s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 90/196 [02:28<02:55,  1.66s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 91/196 [02:30<02:53,  1.66s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 92/196 [02:31<02:52,  1.66s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 93/196 [02:33<02:50,  1.66s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 94/196 [02:34<02:49,  1.66s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 95/196 [02:36<02:47,  1.66s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 96/196 [02:38<02:46,  1.66s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 97/196 [02:39<02:44,  1.66s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 98/196 [02:41<02:42,  1.66s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 99/196 [02:43<02:41,  1.66s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 100/196 [02:44<02:39,  1.66s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 101/196 [02:46<02:37,  1.66s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 102/196 [02:48<02:36,  1.66s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 103/196 [02:49<02:34,  1.66s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 104/196 [02:51<02:32,  1.66s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 105/196 [02:53<02:31,  1.66s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 106/196 [02:54<02:29,  1.66s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 107/196 [02:56<02:28,  1.67s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 108/196 [02:58<02:26,  1.67s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 109/196 [02:59<02:25,  1.67s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 110/196 [03:01<02:23,  1.67s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 111/196 [03:03<02:21,  1.67s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 112/196 [03:04<02:19,  1.67s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 113/196 [03:06<02:18,  1.67s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 114/196 [03:08<02:16,  1.66s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 115/196 [03:09<02:14,  1.66s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 116/196 [03:11<02:13,  1.66s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 117/196 [03:13<02:11,  1.67s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 118/196 [03:14<02:10,  1.67s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 119/196 [03:16<02:08,  1.67s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 120/196 [03:18<02:06,  1.67s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 121/196 [03:19<02:05,  1.67s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 122/196 [03:21<02:03,  1.67s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 123/196 [03:23<02:02,  1.67s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 124/196 [03:24<02:00,  1.67s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 125/196 [03:26<01:59,  1.68s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 126/196 [03:28<01:57,  1.68s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 127/196 [03:30<01:55,  1.68s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 128/196 [03:31<01:54,  1.68s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 129/196 [03:33<01:52,  1.68s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 130/196 [03:35<01:51,  1.69s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 131/196 [03:36<01:49,  1.69s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 132/196 [03:38<01:48,  1.69s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 133/196 [03:40<01:46,  1.69s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 134/196 [03:41<01:44,  1.69s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 135/196 [03:43<01:43,  1.69s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 136/196 [03:45<01:41,  1.69s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 137/196 [03:46<01:39,  1.69s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 138/196 [03:48<01:38,  1.69s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 139/196 [03:50<01:36,  1.69s/it]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 140/196 [03:52<01:35,  1.70s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 141/196 [03:53<01:33,  1.70s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 142/196 [03:55<01:31,  1.70s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 143/196 [03:57<01:29,  1.70s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 144/196 [03:58<01:28,  1.70s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 145/196 [04:00<01:26,  1.69s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 146/196 [04:02<01:24,  1.69s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 147/196 [04:03<01:22,  1.69s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 148/196 [04:05<01:21,  1.69s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 149/196 [04:07<01:19,  1.69s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 150/196 [04:09<01:18,  1.70s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 151/196 [04:10<01:16,  1.70s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 152/196 [04:12<01:14,  1.70s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 153/196 [04:14<01:12,  1.70s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 154/196 [04:15<01:11,  1.70s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 155/196 [04:17<01:09,  1.70s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 156/196 [04:19<01:08,  1.70s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 157/196 [04:20<01:06,  1.70s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 158/196 [04:22<01:04,  1.71s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 159/196 [04:24<01:03,  1.71s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 160/196 [04:26<01:01,  1.71s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 161/196 [04:27<00:59,  1.70s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 162/196 [04:29<00:58,  1.71s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 163/196 [04:31<00:56,  1.71s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 164/196 [04:32<00:54,  1.70s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 165/196 [04:34<00:52,  1.70s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 166/196 [04:36<00:51,  1.70s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 167/196 [04:37<00:49,  1.70s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 168/196 [04:39<00:47,  1.70s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 169/196 [04:41<00:45,  1.69s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 170/196 [04:43<00:44,  1.69s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 171/196 [04:44<00:42,  1.69s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 172/196 [04:46<00:40,  1.69s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 173/196 [04:48<00:38,  1.68s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 174/196 [04:49<00:37,  1.68s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 175/196 [04:51<00:35,  1.68s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 176/196 [04:53<00:33,  1.68s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 177/196 [04:54<00:31,  1.68s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 178/196 [04:56<00:30,  1.68s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 179/196 [04:58<00:28,  1.68s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 180/196 [04:59<00:26,  1.68s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 181/196 [05:01<00:25,  1.68s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 182/196 [05:03<00:23,  1.68s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 183/196 [05:04<00:21,  1.68s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 184/196 [05:06<00:20,  1.67s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 185/196 [05:08<00:18,  1.67s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 186/196 [05:09<00:16,  1.67s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 187/196 [05:11<00:15,  1.67s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 188/196 [05:13<00:13,  1.67s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 189/196 [05:14<00:11,  1.67s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 190/196 [05:16<00:09,  1.67s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 191/196 [05:18<00:08,  1.67s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 192/196 [05:19<00:06,  1.67s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 193/196 [05:21<00:04,  1.67s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 194/196 [05:23<00:03,  1.66s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 195/196 [05:24<00:01,  1.66s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 196/196 [05:25<00:00,  1.36s/it]\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 10000\n",
      "  Batch size = 256\u001b[0m\n",
      "\u001b[34mNum examples = 10000\n",
      "  Batch size = 256\u001b[0m\n",
      "\u001b[34m0%|          | 0/40 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m5%|▌         | 2/40 [00:01<00:23,  1.58it/s]#033[A\u001b[0m\n",
      "\u001b[34m8%|▊         | 3/40 [00:02<00:33,  1.12it/s]#033[A\u001b[0m\n",
      "\u001b[34m10%|█         | 4/40 [00:03<00:37,  1.03s/it]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▎        | 5/40 [00:05<00:38,  1.11s/it]#033[A\u001b[0m\n",
      "\u001b[34m15%|█▌        | 6/40 [00:06<00:39,  1.16s/it]#033[A\u001b[0m\n",
      "\u001b[34m18%|█▊        | 7/40 [00:07<00:39,  1.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m20%|██        | 8/40 [00:08<00:38,  1.21s/it]#033[A\u001b[0m\n",
      "\u001b[34m22%|██▎       | 9/40 [00:10<00:38,  1.23s/it]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▌       | 10/40 [00:11<00:37,  1.24s/it]#033[A\u001b[0m\n",
      "\u001b[34m28%|██▊       | 11/40 [00:12<00:36,  1.25s/it]#033[A\u001b[0m\n",
      "\u001b[34m30%|███       | 12/40 [00:13<00:35,  1.25s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▎      | 13/40 [00:15<00:33,  1.26s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|███▌      | 14/40 [00:16<00:32,  1.26s/it]#033[A\u001b[0m\n",
      "\u001b[34m38%|███▊      | 15/40 [00:17<00:31,  1.26s/it]#033[A\u001b[0m\n",
      "\u001b[34m40%|████      | 16/40 [00:18<00:30,  1.26s/it]#033[A\u001b[0m\n",
      "\u001b[34m42%|████▎     | 17/40 [00:20<00:29,  1.26s/it]#033[A\u001b[0m\n",
      "\u001b[34m45%|████▌     | 18/40 [00:21<00:27,  1.26s/it]#033[A\u001b[0m\n",
      "\u001b[34m48%|████▊     | 19/40 [00:22<00:26,  1.27s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 20/40 [00:24<00:25,  1.27s/it]#033[A\u001b[0m\n",
      "\u001b[34m52%|█████▎    | 21/40 [00:25<00:24,  1.27s/it]#033[A\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 22/40 [00:26<00:22,  1.27s/it]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▊    | 23/40 [00:27<00:21,  1.27s/it]#033[A\u001b[0m\n",
      "\u001b[34m60%|██████    | 24/40 [00:29<00:20,  1.27s/it]#033[A\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 25/40 [00:30<00:19,  1.27s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 26/40 [00:31<00:17,  1.27s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 27/40 [00:32<00:16,  1.27s/it]#033[A\u001b[0m\n",
      "\u001b[34m70%|███████   | 28/40 [00:34<00:15,  1.28s/it]#033[A\u001b[0m\n",
      "\u001b[34m72%|███████▎  | 29/40 [00:35<00:14,  1.28s/it]#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 30/40 [00:36<00:12,  1.28s/it]#033[A\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 31/40 [00:38<00:11,  1.28s/it]#033[A\u001b[0m\n",
      "\u001b[34m80%|████████  | 32/40 [00:39<00:10,  1.28s/it]#033[A\u001b[0m\n",
      "\u001b[34m82%|████████▎ | 33/40 [00:40<00:08,  1.28s/it]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 34/40 [00:41<00:07,  1.28s/it]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 35/40 [00:43<00:06,  1.28s/it]#033[A\u001b[0m\n",
      "\u001b[34m90%|█████████ | 36/40 [00:44<00:05,  1.29s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▎| 37/40 [00:45<00:03,  1.28s/it]#033[A\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 38/40 [00:47<00:02,  1.29s/it]#033[A\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 39/40 [00:48<00:01,  1.29s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 40/40 [00:48<00:00,  1.04it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2357163280248642, 'eval_accuracy': 0.9088, 'eval_f1': 0.9073359073359073, 'eval_precision': 0.9294338051623647, 'eval_recall': 0.8862643906312029, 'eval_runtime': 49.8137, 'eval_samples_per_second': 200.748, 'eval_steps_per_second': 0.803, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 196/196 [06:15<00:00,  1.36s/it]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 40/40 [00:48<00:00,  1.04it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m{'train_runtime': 375.2984, 'train_samples_per_second': 66.614, 'train_steps_per_second': 0.522, 'train_loss': 0.4912820543561663, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 196/196 [06:15<00:00,  1.36s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 196/196 [06:15<00:00,  1.91s/it]\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 10000\n",
      "  Batch size = 256\u001b[0m\n",
      "\u001b[34mNum examples = 10000\n",
      "  Batch size = 256\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit({\"train\": training_input_path, \"test\": test_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the endpoint\n",
    "\n",
    "To deploy our endpoint, we call `deploy()` on our HuggingFace estimator object, passing in our desired number of instances and instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = huggingface_estimator.deploy(initial_instance_count=1, instance_type=\"ml.g4dn.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use the returned predictor object to call the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.984623908996582}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_input = {\n",
    "    \"inputs\": \"This is the best movie ever made in history, an absolute sculpted work of art that depicts every emotion of human existence, from suffering, to courage to love, in front of the background of political astuteness and socio-hierarchal analysis.\"\n",
    "}\n",
    "\n",
    "predictor.predict(sentiment_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.9781230092048645}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_input = {\n",
    "    \"inputs\": \"Another bloated film that gets all the history wrong, turns all of the characters into stick figures and makes piles of money for the star.\"\n",
    "}\n",
    "\n",
    "predictor.predict(sentiment_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we delete the endpoint again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.13 Python 3.9 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.13-cpu-py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
